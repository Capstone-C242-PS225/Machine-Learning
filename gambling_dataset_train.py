# -*- coding: utf-8 -*-
"""gambling_dataset_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/FadQode/6baf2b032817ae09b521d4edb9d9ce29/gambling_dataset_train.ipynb

# Mengolah Dataset Judol
"""


import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler

# gamble_df = pd.read_csv("/content/sample_data/gambling_data.csv")

# gamble_df.head()
# gamble_df.describe()

# gamble_df.drop(columns=['transaction_status', 'transaction_type', 'transactionId',"entryType"], inplace=True)
# gamble_df.isna().sum()

# gamble_df.info()

# features = ["transaction_amount", "user_total_cashout", "user_total_balance", "company_total_cashout", "company_total_balance"]


# # Normalisasi data numerik menggunakan StandardScaler
# scaler = StandardScaler()
# scaled_features = scaler.fit_transform(gamble_df[features])

# # Menentukan jumlah kluster optimal menggunakan metode Elbow
# wcss = []  # Within-Cluster Sum of Squares
# for i in range(1, 11):
#     kmeans = KMeans(n_clusters=i, random_state=42)
#     kmeans.fit(scaled_features)
#     wcss.append(kmeans.inertia_)

# # Plot metode Elbow untuk menemukan jumlah kluster optimal
# plt.plot(range(1, 11), wcss, marker='o')
# plt.title("Metode Elbow untuk Menentukan Jumlah Kluster Optimal")
# plt.xlabel("Jumlah Kluster")
# plt.ylabel("Within-Cluster Sum of Squares (WCSS)")
# plt.show()

# # Melanjutkan dengan jumlah kluster optimal (misalnya, 3 kluster)
# optimal_clusters = 4
# kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
# gamble_df["addiction"] = kmeans.fit_predict(scaled_features)

# # Simpan dataset yang sudah diberi label ke dalam file CSV
# gamble_df.to_csv("gambling_clustered_data.csv", index=False)

# # Tampilkan hasil clustering
# print(gamble_df.head())

# gamble_df.info()

# pca = PCA(n_components=2)
# reduced_features = pca.fit_transform(scaled_features)

# # Plot hasil PCA untuk melihat persebaran kluster
# plt.figure(figsize=(10, 6))
# for addiction in range(optimal_clusters):
#     plt.scatter(
#         reduced_features[gamble_df["addiction"] == addiction, 0],
#         reduced_features[gamble_df["addiction"] == addiction, 1],
#         label=f'Cluster {addiction}'
#     )

# plt.title("Visualisasi Kluster Menggunakan PCA")
# plt.xlabel("Komponen Principal 1")
# plt.ylabel("Komponen Principal 2")
# plt.legend()
# plt.show()

# # Menghitung jumlah data dalam setiap kluster
# addiction_counts = gamble_df.groupby("addiction").size()

# # Menampilkan jumlah data di setiap kluster
# print("Jumlah data di setiap kluster:")
# print(addiction_counts)

# gamble_df.describe()

# df = pd.read_csv('gambling_clustered_data.csv')

# # Mengubah fitur boolean menjadi numerik
# df['newRegister'] = df['newRegister'].astype(int)

# categorical_features = ['status']
# encoder = OneHotEncoder(sparse_output=False)
# encoded_features = encoder.fit_transform(df[categorical_features])

# encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))
# df = pd.concat([df.drop(columns=categorical_features), encoded_df], axis=1)


# scaler = MinMaxScaler()
# numerical_features = ['transaction_amount', 'user_total_cashout', 'user_total_balance',
#                       'company_total_cashout', 'company_total_balance']

# df[numerical_features] = scaler.fit_transform(df[numerical_features])
# # Memisahkan data menjadi fitur dan label
# X = df.drop(columns=['addiction'])
# y = df['addiction']

# # Split data menjadi training set dan test set
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, shuffle = True)

# print("Data siap digunakan untuk training!")

# df.describe()

# # Pisahkan fitur dan label
# X = df.drop(columns=['addiction'])
# y = df['addiction']

# model = Sequential([
#     Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
#     Dense(32, activation='relu'),
#     Dense(10, activation='relu'),
#     Dense(len(y.unique()), activation='softmax')  # Output sesuai jumlah addiction
# ])

# # Kompilasi model
# model.compile(optimizer='adam',
#               loss='sparse_categorical_crossentropy',
#               metrics=['accuracy'])

# # Training model
# history = model.fit(X_train, y_train,
#                     epochs=50,
#                     validation_split=0.2,
#                     batch_size=16)

# # Evaluasi model
# test_loss, test_acc = model.evaluate(X_test, y_test)
# print(f"Akurasi pada test set: {test_acc * 100:.2f}%")

# # Simpan Model
# model.save("addiction_predict.h5")



#WITH SCALING

from tensorflow.keras.models import load_model
# Load the trained model
model = load_model('./API/addiction_predict.h5')
import joblib


def classify(cluster):
    if cluster == 0:
        return 'adiksi ringan'
    elif cluster == 1:
        return 'adiksi sedengan'
    elif cluster == 2:
        return 'adiksi berat'
    else:
        return 'adiksi sangat berat'
def predict_addiction():

    data = [
        [1, 55000, 93000, -93000, 298724274.0, 20496113, 1],
        [1, 50000, 50000, -50000, 0.0, 120436851, 1],
        [1, 100000, 11417000, 483000, 197032923.0, 1215688158, 1],
        [1, 77000, 4041000, -1390000, 49894832.33, 186107146, 1],
        [1, 25000, 710000, 350000, 197032923.0, 1215688158, 1],
        [1, 99000, 74735100, -12246100, 69041004.0, 126147391, 1],
        [1, 11000, 371200, -371200, 0.0, 81000, 1],
        [1, 50000, 50000, -50000, 197032923.0, 1215688158, 1],
        [1, 50000, 445500, -345500, 0.0, 120436851, 1],
        [1, 50000, 39579765, -3695000, 0.0, 120436851, 1]
    ]



    scaler1 = StandardScaler()
    scaled_data1 = scaler1.fit_transform(data) 

    scaler2 = MinMaxScaler()
    scaled_data2 = scaler2.fit_transform(scaled_data1)

    input_data = np.array(scaled_data2)


    predictions = model.predict(input_data)


    predicted_classes = np.argmax(predictions, axis=1)

    for i, prediction in enumerate(predicted_classes):
        print(f"Prediction for data point {i+1}: Class {classify(prediction)}")
        print(f"Prediction probabilities: {predictions[i]}")
        print()

    joblib.dump(scaler1, 'scaler_standard.pkl')
    joblib.dump(scaler2, 'scaler_minmax.pkl')

predict_addiction()

def classify(cluster):
    if cluster == 0:
        return 'adiksi ringan'
    elif cluster == 1:
        return 'adiksi sedengan'
    elif cluster == 2:
        return 'adiksi berat'
    else:
        return 'adiksi sangat berat'

def predict_addiction():
    # Example data for prediction (all columns excluding 'SUCCESS' and 'True')
    data = [
        [1, 55000, 93000, -93000, 298724274.0, 20496113, 1],
        [1, 50000, 50000, -50000, 0.0, 120436851, 1],
        [1, 100000, 11417000, 483000, 197032923.0, 1215688158, 1],
        [1, 77000, 4041000, -1390000, 49894832.33, 186107146, 1],
        [1, 25000, 710000, 350000, 197032923.0, 1215688158, 1],
        [1, 99000, 74735100, -12246100, 69041004.0, 126147391, 1],
        [1, 11000, 371200, -371200, 0.0, 81000, 1],
        [1, 50000, 50000, -50000, 197032923.0, 1215688158, 1],
        [1, 50000, 445500, -345500, 0.0, 120436851, 1],
        [1, 50000, 39579765, -3695000, 0.0, 120436851, 1]
    ]




    # Step 3: Prepare input data
    input_data = np.array(data)

    # Step 4: Predict using the trained model
    predictions = model.predict(input_data)

    # Step 5: Get the predicted class for each data point
    predicted_classes = np.argmax(predictions, axis=1)

    # Print the results for all 10 data points
    for i, prediction in enumerate(predicted_classes):
        print(f"Prediction for data point {i+1}: Class {classify(prediction)}")
        print(f"Prediction probabilities: {predictions[i]}")
        print()

# Call the function to predict the 10 data points
predict_addiction()

